{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import gmtime, strftime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score,roc_curve,precision_score, recall_score, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRFClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv('E:/ML_IP/ML_projects/datastorm/credit_card_default_train.csv')\n",
    "test_data = pd.read_csv('E:/ML_IP/ML_projects/datastorm/credit_card_default_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE','PAY_JULY','PAY_AUG','PAY_SEP','PAY_OCT','PAY_NOV','PAY_DEC']\n",
    "target = 'NEXT_MONTH_DEFAULT'\n",
    "ID = 'Client_ID'\n",
    "num_cols = [col for col in train_data.columns.tolist() if col not in cat_cols +[target]+[ID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Paid_Due_July(row):\n",
    "    if row['PAID_AMT_JULY'] == 0:\n",
    "        val = row['DUE_AMT_JULY']\n",
    "    else:\n",
    "        val = row['DUE_AMT_JULY']/row['PAID_AMT_JULY']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Aug(row):\n",
    "    if row['PAID_AMT_AUG'] == 0:\n",
    "        val = row['DUE_AMT_AUG']\n",
    "    else:\n",
    "        val = row['DUE_AMT_AUG']/row['PAID_AMT_AUG']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Sep(row):\n",
    "    if row['PAID_AMT_SEP'] == 0:\n",
    "        val = row['DUE_AMT_SEP']\n",
    "    else:\n",
    "        val = row['DUE_AMT_SEP']/row['PAID_AMT_SEP']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Oct(row):\n",
    "    if row['PAID_AMT_OCT'] == 0:\n",
    "        val = row['DUE_AMT_OCT']\n",
    "    else:\n",
    "        val = row['DUE_AMT_OCT']/row['PAID_AMT_OCT']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Nov(row):\n",
    "    if row['PAID_AMT_NOV'] == 0:\n",
    "        val = row['DUE_AMT_NOV']\n",
    "    else:\n",
    "        val = row['DUE_AMT_NOV']/row['PAID_AMT_NOV']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Dec(row):\n",
    "    if row['PAID_AMT_DEC'] == 0:\n",
    "        val = row['DUE_AMT_DEC']\n",
    "    else:\n",
    "        val = row['DUE_AMT_DEC']/row['PAID_AMT_DEC']\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['PAID_DUE_JULY'] = train_data.apply(Paid_Due_July, axis=1)\n",
    "train_data['PAID_DUE_AUG'] = train_data.apply(Paid_Due_Aug, axis=1)\n",
    "train_data['PAID_DUE_SEP'] = train_data.apply(Paid_Due_Sep, axis=1)\n",
    "train_data['PAID_DUE_OCT'] = train_data.apply(Paid_Due_Oct, axis=1)\n",
    "train_data['PAID_DUE_NOV'] = train_data.apply(Paid_Due_Nov, axis=1)\n",
    "train_data['PAID_DUE_DEC'] = train_data.apply(Paid_Due_Dec, axis=1)\n",
    "\n",
    "test_data['PAID_DUE_JULY'] = test_data.apply(Paid_Due_July, axis=1)\n",
    "test_data['PAID_DUE_AUG'] = test_data.apply(Paid_Due_Aug, axis=1)\n",
    "test_data['PAID_DUE_SEP'] = test_data.apply(Paid_Due_Sep, axis=1)\n",
    "test_data['PAID_DUE_OCT'] = test_data.apply(Paid_Due_Oct, axis=1)\n",
    "test_data['PAID_DUE_NOV'] = test_data.apply(Paid_Due_Nov, axis=1)\n",
    "test_data['PAID_DUE_DEC'] = test_data.apply(Paid_Due_Dec, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['PAY_TOT'] = train_data['PAY_JULY'] + train_data['PAY_AUG'] + train_data['PAY_SEP'] + train_data['PAY_OCT'] + train_data['PAY_NOV'] + train_data['PAY_DEC']\n",
    "test_data['PAY_TOT'] = test_data['PAY_JULY'] + test_data['PAY_AUG'] + test_data['PAY_SEP'] + test_data['PAY_OCT'] + test_data['PAY_NOV'] + test_data['PAY_DEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isZero(row):\n",
    "    if row['PAY_TOT'] == 0:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isZero(row):\n",
    "    if row['PAY_TOT'] == 0:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "\n",
    "train_df = pd.DataFrame(enc.fit_transform(train_data[['Balance_Limit_V1','EDUCATION_STATUS','MARITAL_STATUS','AGE','Gender']]).toarray())\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "\n",
    "test_df = pd.DataFrame(enc.fit_transform(test_data[['Balance_Limit_V1','EDUCATION_STATUS','MARITAL_STATUS','AGE','Gender']]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data,train_df],axis=1)\n",
    "test_data = pd.concat([test_data,test_df],axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data.drop([target,ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1), \n",
    "                                                    train_data[target], test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     13029\n",
      "           1       0.70      0.38      0.49      3771\n",
      "\n",
      "    accuracy                           0.82     16800\n",
      "   macro avg       0.77      0.67      0.69     16800\n",
      "weighted avg       0.81      0.82      0.80     16800\n",
      "\n",
      "\n",
      "\n",
      "[[12418   611]\n",
      " [ 2334  1437]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89      5641\n",
      "           1       0.67      0.37      0.47      1559\n",
      "\n",
      "    accuracy                           0.82      7200\n",
      "   macro avg       0.76      0.66      0.68      7200\n",
      "weighted avg       0.81      0.82      0.80      7200\n",
      "\n",
      "\n",
      "\n",
      "[[5357  284]\n",
      " [ 985  574]]\n",
      "Test_Accuracy = 0.82375\n"
     ]
    }
   ],
   "source": [
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "params_best = {}\n",
    "params_best['learning_rate'] = 0.1\n",
    "params_best['boosting_type'] = 'dart'\n",
    "params_best['objective'] = 'binary'\n",
    "params_best['metric'] = 'binary_logloss'\n",
    "params_best['sub_feature'] = 0.7\n",
    "params_best['num_leaves'] = 18\n",
    "params_best['min_data'] = 70\n",
    "params_best['max_depth'] = 510\n",
    "params_best['max_bin']=120\n",
    "params_best['n_estimators']=110\n",
    "params_best['colsample_bytree' ]=0\n",
    "\n",
    "\n",
    "clf = lgb.train(params_best, d_train, 100)\n",
    "\n",
    "#Prediction\n",
    "test_probs = clf.predict(x_test)\n",
    "\n",
    "\n",
    "def avoid_prob(preds_lgb):\n",
    "    for i in range(len(preds_lgb)):\n",
    "        if preds_lgb[i]>=0.5:       # setting threshold to .5\n",
    "            preds_lgb[i]=1\n",
    "        else:\n",
    "            preds_lgb[i]=0\n",
    "    return preds_lgb\n",
    "\n",
    "test_preds=avoid_prob(test_probs)\n",
    "train_preds=avoid_prob(clf.predict(x_train))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score,roc_curve\n",
    "\n",
    "print(classification_report(y_train,train_preds))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(y_train,train_preds))\n",
    "#####\n",
    "test_preds=avoid_prob(test_probs)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score,roc_curve\n",
    "\n",
    "print(classification_report(y_test,test_preds))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(y_test,test_preds))\n",
    "\n",
    "print(\"Test_Accuracy = {}\".format(accuracy_score(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['boosting_type', 'class_weight', 'colsample_bytree', 'importance_type', 'learning_rate', 'max_depth', 'min_child_samples', 'min_child_weight', 'min_split_gain', 'n_estimators', 'n_jobs', 'num_leaves', 'objective', 'random_state', 'reg_alpha', 'reg_lambda', 'silent', 'subsample', 'subsample_for_bin', 'subsample_freq'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use gridsearchcv\n",
    "\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          objective = 'binary', \n",
    "          n_jobs = 5, \n",
    "          silent = True,\n",
    "          max_depth = 10)\n",
    "\n",
    "# To view the default model parameters:\n",
    "mdl.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    3.8s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'dart', 'learning_rate': 0.1, 'max_bin': 120, 'max_depth': 510, 'metric': 'binary_logloss', 'min_data': 70, 'n_estimators': 110, 'num_leaves': 18, 'objective': 'binary', 'sub_feature': 0.7}\n",
      "0.8172619047619047\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params['learning_rate'] = [0.1]\n",
    "params['boosting_type'] = ['dart']\n",
    "params['objective'] = ['binary']\n",
    "params['metric'] = ['binary_logloss']\n",
    "params['sub_feature'] = [0.7]\n",
    "params['num_leaves'] = [18]\n",
    "params['min_data'] = [70]\n",
    "params['max_depth'] = [510]\n",
    "params['max_bin']=[120]\n",
    "params['n_estimators']=[110]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(mdl, params, verbose=1, cv=4, n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(x_train,y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "probs=grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_best = {}\n",
    "params_best['learning_rate'] = 0.1\n",
    "params_best['boosting_type'] = 'dart'\n",
    "params_best['objective'] = 'binary'\n",
    "params_best['metric'] = 'binary_logloss'\n",
    "params_best['sub_feature'] = 0.5\n",
    "params_best['num_leaves'] = 18\n",
    "params_best['min_data'] = 70\n",
    "params_best['max_depth'] = 5\n",
    "params_best['max_bin']=250\n",
    "params_best['n_estimators']=110\n",
    "params_best['colsample_bytree' ]=0\n",
    "params_best['subsample']=0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=grid.predict(test_data.drop([ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1))\n",
    "sample_submission = pd.DataFrame(columns=[ID,target])\n",
    "sample_submission[ID]=test_data[ID]\n",
    "sample_submission[target] = probs\n",
    "sample_submission.set_index(ID)\n",
    "submission = sample_submission.to_csv('data-storm-day2-2.csv',index = None)\n",
    "\n",
    "\n",
    "#submission was in float32 format. It has been correcteed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
