{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Relevant Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBRFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('credit_card_default_train.csv')\n",
    "test_data = pd.read_csv('credit_card_default_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE','PAY_JULY','PAY_AUG','PAY_SEP','PAY_OCT','PAY_NOV','PAY_DEC']\n",
    "target = 'NEXT_MONTH_DEFAULT'\n",
    "ID = 'Client_ID'\n",
    "num_cols = [col for col in train_data.columns.tolist() if col not in cat_cols +[target]+[ID]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTwo_July(row):\n",
    "    if row['PAY_JULY'] >= 2:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "train_data['isTwo_JULY'] = train_data.apply(isTwo_July, axis=1)\n",
    "test_data['isTwo_JULY'] = train_data.apply(isTwo_July, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Paid_Due_July(row):\n",
    "    if row['PAID_AMT_JULY'] == 0:\n",
    "        val = row['DUE_AMT_JULY']\n",
    "    else:\n",
    "        val = row['DUE_AMT_JULY']/row['PAID_AMT_JULY']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Aug(row):\n",
    "    if row['PAID_AMT_AUG'] == 0:\n",
    "        val = row['DUE_AMT_AUG']\n",
    "    else:\n",
    "        val = row['DUE_AMT_AUG']/row['PAID_AMT_AUG']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Sep(row):\n",
    "    if row['PAID_AMT_SEP'] == 0:\n",
    "        val = row['DUE_AMT_SEP']\n",
    "    else:\n",
    "        val = row['DUE_AMT_SEP']/row['PAID_AMT_SEP']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Oct(row):\n",
    "    if row['PAID_AMT_OCT'] == 0:\n",
    "        val = row['DUE_AMT_OCT']\n",
    "    else:\n",
    "        val = row['DUE_AMT_OCT']/row['PAID_AMT_OCT']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Nov(row):\n",
    "    if row['PAID_AMT_NOV'] == 0:\n",
    "        val = row['DUE_AMT_NOV']\n",
    "    else:\n",
    "        val = row['DUE_AMT_NOV']/row['PAID_AMT_NOV']\n",
    "    return val\n",
    "\n",
    "def Paid_Due_Dec(row):\n",
    "    if row['PAID_AMT_DEC'] == 0:\n",
    "        val = row['DUE_AMT_DEC']\n",
    "    else:\n",
    "        val = row['DUE_AMT_DEC']/row['PAID_AMT_DEC']\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['PAID_DUE_JULY'] = train_data.apply(Paid_Due_July, axis=1)\n",
    "train_data['PAID_DUE_AUG'] = train_data.apply(Paid_Due_Aug, axis=1)\n",
    "train_data['PAID_DUE_SEP'] = train_data.apply(Paid_Due_Sep, axis=1)\n",
    "train_data['PAID_DUE_OCT'] = train_data.apply(Paid_Due_Oct, axis=1)\n",
    "train_data['PAID_DUE_NOV'] = train_data.apply(Paid_Due_Nov, axis=1)\n",
    "train_data['PAID_DUE_DEC'] = train_data.apply(Paid_Due_Dec, axis=1)\n",
    "\n",
    "test_data['PAID_DUE_JULY'] = test_data.apply(Paid_Due_July, axis=1)\n",
    "test_data['PAID_DUE_AUG'] = test_data.apply(Paid_Due_Aug, axis=1)\n",
    "test_data['PAID_DUE_SEP'] = test_data.apply(Paid_Due_Sep, axis=1)\n",
    "test_data['PAID_DUE_OCT'] = test_data.apply(Paid_Due_Oct, axis=1)\n",
    "test_data['PAID_DUE_NOV'] = test_data.apply(Paid_Due_Nov, axis=1)\n",
    "test_data['PAID_DUE_DEC'] = test_data.apply(Paid_Due_Dec, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['PAY_TOT'] = train_data['PAY_JULY'] + train_data['PAY_AUG'] + train_data['PAY_SEP'] + train_data['PAY_OCT'] + train_data['PAY_NOV'] + train_data['PAY_DEC']\n",
    "test_data['PAY_TOT'] = test_data['PAY_JULY'] + test_data['PAY_AUG'] + test_data['PAY_SEP'] + test_data['PAY_OCT'] + test_data['PAY_NOV'] + test_data['PAY_DEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isZero(row):\n",
    "    if row['PAY_TOT'] ==0:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['PAY_TOT_0'] = train_data.apply(isZero, axis=1)\n",
    "test_data['PAY_TOT_0'] = test_data.apply(isZero, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "\n",
    "train_df = pd.DataFrame(enc.fit_transform(train_data[['Balance_Limit_V1','EDUCATION_STATUS','MARITAL_STATUS','AGE','Gender']]).toarray())\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "\n",
    "test_df = pd.DataFrame(enc.fit_transform(test_data[['Balance_Limit_V1','EDUCATION_STATUS','MARITAL_STATUS','AGE','Gender']]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data,train_df],axis=1)\n",
    "test_data = pd.concat([test_data,test_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop([target,ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X))\n",
    "y = train_data[target]\n",
    "\n",
    "test = test_data.drop([ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1)\n",
    "scaler = StandardScaler().fit(test)\n",
    "test = pd.DataFrame(scaler.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . learning_rate=0.1, n_estimators=100, score=0.811, total=   2.2s\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . learning_rate=0.1, n_estimators=100, score=0.824, total=   2.2s\n",
      "[CV] learning_rate=0.1, n_estimators=100 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . learning_rate=0.1, n_estimators=100, score=0.823, total=   2.2s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV] . learning_rate=0.1, n_estimators=200, score=0.811, total=   4.4s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV] . learning_rate=0.1, n_estimators=200, score=0.823, total=   4.5s\n",
      "[CV] learning_rate=0.1, n_estimators=200 .............................\n",
      "[CV] . learning_rate=0.1, n_estimators=200, score=0.822, total=   4.3s\n",
      "[CV] learning_rate=0.1, n_estimators=400 .............................\n",
      "[CV] . learning_rate=0.1, n_estimators=400, score=0.810, total=   8.6s\n",
      "[CV] learning_rate=0.1, n_estimators=400 .............................\n",
      "[CV] . learning_rate=0.1, n_estimators=400, score=0.823, total=   8.6s\n",
      "[CV] learning_rate=0.1, n_estimators=400 .............................\n",
      "[CV] . learning_rate=0.1, n_estimators=400, score=0.823, total=   8.6s\n",
      "[CV] learning_rate=0.01, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=100, score=0.811, total=   2.3s\n",
      "[CV] learning_rate=0.01, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=100, score=0.824, total=   2.3s\n",
      "[CV] learning_rate=0.01, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=100, score=0.823, total=   2.8s\n",
      "[CV] learning_rate=0.01, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=200, score=0.811, total=   6.1s\n",
      "[CV] learning_rate=0.01, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=200, score=0.823, total=   4.5s\n",
      "[CV] learning_rate=0.01, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=200, score=0.822, total=   4.4s\n",
      "[CV] learning_rate=0.01, n_estimators=400 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=400, score=0.810, total=   8.6s\n",
      "[CV] learning_rate=0.01, n_estimators=400 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=400, score=0.823, total=   8.5s\n",
      "[CV] learning_rate=0.01, n_estimators=400 ............................\n",
      "[CV]  learning_rate=0.01, n_estimators=400, score=0.823, total=   8.6s\n",
      "[CV] learning_rate=0.03, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=100, score=0.811, total=   2.2s\n",
      "[CV] learning_rate=0.03, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=100, score=0.824, total=   2.2s\n",
      "[CV] learning_rate=0.03, n_estimators=100 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=100, score=0.823, total=   2.2s\n",
      "[CV] learning_rate=0.03, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=200, score=0.811, total=   4.3s\n",
      "[CV] learning_rate=0.03, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=200, score=0.823, total=   4.3s\n",
      "[CV] learning_rate=0.03, n_estimators=200 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=200, score=0.822, total=   4.3s\n",
      "[CV] learning_rate=0.03, n_estimators=400 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=400, score=0.810, total=   9.0s\n",
      "[CV] learning_rate=0.03, n_estimators=400 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=400, score=0.823, total=   8.9s\n",
      "[CV] learning_rate=0.03, n_estimators=400 ............................\n",
      "[CV]  learning_rate=0.03, n_estimators=400, score=0.823, total=   8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=XGBRFClassifier(base_score=0.5, colsample_bylevel=1,\n",
       "                                       colsample_bynode=0.8, colsample_bytree=1,\n",
       "                                       gamma=0, learning_rate=1,\n",
       "                                       max_delta_step=0, max_depth=3,\n",
       "                                       min_child_weight=1, missing=None,\n",
       "                                       n_estimators=100, n_jobs=1, nthread=None,\n",
       "                                       objective='binary:logistic',\n",
       "                                       random_state=0, reg_alpha=0,\n",
       "                                       reg_lambda=1, scale_pos_weight=1,\n",
       "                                       seed=None, silent=None, subsample=0.8,\n",
       "                                       verbosity=1),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.03],\n",
       "                         'n_estimators': [100, 200, 400]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid = {'n_estimators': [100,200,400],\n",
    "               'learning_rate': [0.1,0.01,0.03]}\n",
    "\n",
    "grid = GridSearchCV(XGBRFClassifier(),random_grid,refit=True,verbose=3,cv=3)\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 100}\n",
      "0.8193333333333334\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.805625   0.81270833 0.82458333 0.83083333 0.82145833]\n",
      "0.8190416666666666\n"
     ]
    }
   ],
   "source": [
    "model_xgb = XGBRFClassifier(n_estimators=400,learning_rate=0.1)\n",
    "#model_rf.fit(train_data.drop([target,ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1),train_data[target])\n",
    "#model_rf.fit(X_train,y_train)\n",
    "scores = cross_val_score(model_xgb, X, y, cv=5, scoring='accuracy')\n",
    "print(scores)\n",
    "print (np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRFClassifier(base_score=0.5, colsample_bylevel=1, colsample_bynode=0.8,\n",
       "                colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "                max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "                n_estimators=400, n_jobs=1, nthread=None,\n",
       "                objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "                reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "                subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop([target,ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1), \n",
    "                                                    train_data[target], test_size=0.30)\n",
    "model_xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89      5551\n",
      "           1       0.69      0.34      0.46      1649\n",
      "\n",
      "    accuracy                           0.81      7200\n",
      "   macro avg       0.76      0.65      0.67      7200\n",
      "weighted avg       0.80      0.81      0.79      7200\n",
      "\n",
      "\n",
      "\n",
      "[[5295  256]\n",
      " [1082  567]]\n"
     ]
    }
   ],
   "source": [
    "preds_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,preds_xgb))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(y_test,preds_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the model in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRFClassifier(base_score=0.5, colsample_bylevel=1, colsample_bynode=0.8,\n",
       "                colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "                max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "                n_estimators=400, n_jobs=1, nthread=None,\n",
       "                objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "                reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "                subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBRFClassifier(n_estimators=400,learning_rate = 0.1)\n",
    "model_xgb.fit(train_data.drop([target,ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1),train_data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_xgb = model_xgb.predict(test_data.drop([ID,'Balance_Limit_V1','Gender','EDUCATION_STATUS','MARITAL_STATUS','AGE'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.DataFrame(columns=[ID,target])\n",
    "sample_submission[ID]=test_data[ID]\n",
    "sample_submission[target] = preds_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.to_csv('data-storm-day1-3.csv',index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
